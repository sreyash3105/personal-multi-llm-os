Personal Local Multi-LLM Operating System (Local AI OS)

A local-first, privacy-preserving AI assistant that runs entirely on your own machine â€” no cloud, no API keys, no external data sharing.

This system acts as a desktop AI OS with a code pipeline, study mode, multimodal chat, vision AI, per-profile memory, tools runtime, and an observability dashboard â€” all powered by local LLMs via Ollama.

ğŸš€ Core Features
Module	Description
ğŸ”§ Code AI	Coder â†’ Reviewer â†’ Judge â†’ Final decision cycle
ğŸ“š Study Mode	///short, ///deep, ///quiz, or default teaching
ğŸ§  Multimodal Chat	Multi-profile, multi-chat workspace with model overrides
ğŸ‘ Vision AI	OCR / UI debug / code from screenshot / detailed image description
ğŸ§© Tools Runtime	Run local tools from chat via ///tool and ///tool+chat
ğŸ—„ Memory	Per-profile knowledge base (SQLite) with context retrieval
ğŸ“Š Dashboard	Full trace of every AI interaction (coder / reviewer / judge flow)
ğŸ”’ Offline	Nothing leaves your device â€” no cloud calls at any stage
ğŸ— Architecture Overview
Client (Laptop / Browser)
        â”‚
LAN HTTP
        â”‚
FastAPI Backend  â†â†’  SQLite (Chat / KB / History)
        â”‚
        â”œâ”€â”€ Code Pipeline (Coder / Reviewer / Judge)
        â”œâ”€â”€ Study Pipeline
        â”œâ”€â”€ Vision Pipeline (LLaVA)
        â”œâ”€â”€ Tools Runtime (local functions and utilities)
        â””â”€â”€ Multimodal Chat Workspace
                 â€¢ Profiles
                 â€¢ Chats
                 â€¢ Vision messages
                 â€¢ Tool triggers


Ollama provides all model execution â€” no remote inference:

qwen2.5-coder
codestral
llama3.1
qwen2.5 (14B)
llava (vision)
phi3
deepseek-coder-v2
...

ğŸ–¥ Running Locally
Requirements

Python 3.10+

Ollama installed

At least one LLM pulled (example: ollama pull qwen2.5-coder:7b)

Setup
pip install -r requirements.txt

Launch
uvicorn code_server:app --host 0.0.0.0 --port 8000

Interfaces
URL	Purpose
http://localhost:8000/chat	Multimodal chat workspace
http://localhost:8000/dashboard	Trace dashboard (history of all interactions)
âš™ Tools Runtime â€” Example

Inside chat, execute local tools directly:

///tool ping
{"message": "hello world"}


or with hybrid mode:

///tool+chat system_info


â†’ runs the tool and then asks a stronger model to summarize the result.

ğŸ§  Vision Examples
/api/vision + image â†’ OCR text
/api/vision + image â†’ UI debugging
///vision inside chat (if enabled)


Vision messages are stored as captions, not raw base64 â€” keeping chat context clean.

ğŸ” Privacy & Local-Only Design

This repository does not include:

personal chat logs

history records

knowledge base content

internal documentation

All data stays local and never leaves your machine.

ğŸ“ License

Personal-Use Open License (non-commercial)
You may:

download

modify

run locally

build your own local AI OS

You may not:

resell

commercialize

host as a paid service

Commercial licensing is possible if needed.

âš  Disclaimer

This is a personal research project, actively evolving.
Expect fast development, experimental modules, and refactoring across versions.

â­ Contributions / Forking

Anyone is welcome to:

fork the repo

learn from the architecture

build their own personal AI OS on top of it

PRs may be reviewed based on available time.

ğŸ§­ Roadmap (High-Level)
Version	Goal
V3.2	Multimodal chat + tools + vision
V3.4	Schedulers & automation
V3.6	Embedding-based memory
V4.0	Multi-agent orchestration (optional)

If you like this project, star the repo â­
and feel free to share what you build with it.

End of README