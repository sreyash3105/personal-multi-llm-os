Response
Based on comprehensive analysis of the V5 codebase, I'll now generate the deep internal documentation.
________________________________________
AI-OS V5 Internal Reference Document
1. System Philosophy & Invariants
Core Guarantees
The system maintains these invariants through intentional design:
Local-First Execution
•	All core operations work offline (chat, code, tools, knowledge base)
•	Ollama (local LLM) is the primary inference engine
•	Cloud APIs are optional augmentations, not dependencies
Determinism Over Speed
•	Explicit decisions over hidden heuristics
•	All math and scoring is readable and reviewable
•	No learned models or neural networks outside the LLM itself
Human-in-the-Loop
•	Security sessions require user approval for high-risk operations
•	No autonomous code execution without confirmation
•	Automation can only execute when execute=True is explicitly passed
Replaceability
•	Each layer (input, routing, planning, execution, memory) is independently swappable
•	Models are interchangeable (CODER, REVIEWER, JUDGE models decoupled)
•	No business logic tied to specific model names or vendors
Git as Source of Truth
•	Configuration is code (config.py), not files
•	All meaningful changes must be committed
•	History provides audit trail but Git provides authoritative memory
Strict vs Best-Effort Boundaries
Strict (Fail-Closed)
•	Security engine evaluation failures → BLOCK (auth_level 6)
•	SQLite transactions (WAL journal, foreign keys)
•	Profile data isolation (chat storage, KB, sessions)
•	VRAM limits (MAX_CONCURRENT_HEAVY_REQUESTS)
•	Singleton thread-safety (Planner, SecurityEngine)
Best-Effort (Fail-Safe)
•	Risk assessment failures → default to risk_level 1.0 (minor)
•	Logging/telemetry failures → silently absorbed, never break main flow
•	LLM timeout/retry → exponential backoff with eventual failure
•	Optional dependencies → degrade gracefully, don't crash
Intentional Assumptions
AI-GPU Assumptions
•	Single GPU with 8GB VRAM
•	Ollama manages VRAM allocation
•	MAX_CONCURRENT_HEAVY_REQUESTS=2 prevents OOM for 7B models
Latency Assumptions
•	Ollama API latency is acceptable for interactive use
•	120s timeout covers slow models (DeepSeek 7B)
•	Tool execution completes in under 60s
Locality Assumptions
•	SQLite databases are on fast local storage
•	Network operations are only for optional features
•	File system operations are fast relative to LLM calls
________________________________________
2. High-Level Architecture (As Implemented)
Actual Layers (Not Theoretical)



│                    INPUT LAYER                             │
│  ghost_agent.py (audio, keyboard)                         │
│  code_server.py (HTTP API)                                │
└──────────────────────────┬──────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────────┐
│                  ROUTING LAYER                            │
│  router.py (intent classification)                         │
│  - Rule-based patterns                                        │
│  - LLM-based scoring                                         │
└──────────────────────────┬──────────────────────────────────────┘
                           │
         ┌───────────────┴───────────────┬───────────────┐
         ▼                               ▼               ▼
┌────────────────┐            ┌────────────────┐  ┌────────────────┐
│ PLANNING      │            │  EXECUTION     │  │  MEMORY        │
│ LAYER          │            │  LAYER         │  │  LAYER         │
│ planner.py     │            │  - code/      │  │  - chat/      │
│                │            │    pipeline.py │  │    storage.py  │
│                │            │  - tools/     │  │  - kb/        │
│                │            │    runtime.py │  │    profile_kb.py│
│                │            │  - auto/      │  │  - history.py │
│                │            │    executor.py │  │                 │
│                │            │  - vision/    │  │                 │
│                │            │    pipeline.py │  │                 │
│                │            │                │  │                 │
└────────────────┘            └────────────────┘  └────────────────┘
         │
         ▼
┌─────────────────────────────────────────────────────────────┐
│              OBSERVABILITY LAYER                          │
│  - telemetry/history.py (logging)                         │
│  - telemetry/dashboard.py (HTML viz)                      │
│  - telemetry/risk.py (risk scoring)                       │
└─────────────────────────────────────────────────────────────┘


Highly Replaceable
•	planner.py → Alternative intent detection system
•	code/pipeline.py → Alternative code generation workflow
•	vision/pipeline.py → Different vision model or API
•	history.py → Different storage backend
Moderately Replaceable
•	tools_runtime.py → Different tool registry/execution model
•	automation/executor.py → Different automation framework
•	router.py → Alternative routing strategy
Harder to Replace
•	security_engine.py → Tightly integrated with tools_runtime
•	queue_manager.py → All heavy operations depend on job queue
•	ghost_agent.py → Tightly coupled to overlay/process spawning
Boundary Existence Rationale
Routing vs Execution Separation
•	Router only classifies intent, never executes
•	Prevents routing logic from being polluted by execution concerns
•	Allows different execution paths for same intent
Planning vs Execution Separation
•	Planner generates plans but never calls tools directly
•	Ensures observability (plan logged before execution)
•	Enables dry-run mode (execute=False)
Security vs Enforcement Separation
•	SecurityEngine evaluates risk and suggests auth levels
•	ToolsRuntime enforces (or logs) based on config
•	Non-blocking enforcement allows UI to prompt user
Memory vs Processing Separation
•	History logs everything but never influences execution
•	KB provides context but doesn't block execution
•	Ensures audit trail doesn't affect system behavior
________________________________________
3. Execution Flows (DEEP)
3.1 Code Pipeline End-to-End
Entry: /api/code → run_smart_code_pipeline()
┌─────────────────────────────────────────────────────────────┐
│ 1. JOB ENQUEUE (pipeline.py:368)                      │
│    enqueue_job(profile_id, kind="code_smart", is_heavy=True)│
│    → Job created, added to per-profile FIFO queue          │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ 2. VRAM SLOT ACQUISITION (pipeline.py:371-380)     │
│    try_acquire_next_job(profile_id)                     │
│    → Check: Is this profile already running?              │
│    → Check: Is _ACTIVE_HEAVY_COUNT >= 2?              │
│    → If not busy: Activate job, mark as "running"       │
│    → Else: Poll every 0.1s (NO TIMEOUT)              │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ 3. CODER STAGE (pipeline.py:386-215)                  │
│    _HEAVY_SEMAPHORE.acquire() ← VRAM GUARD          │
│    _get_profile_lock(profile_id).acquire() ← SERIALISM   │
│    _run_with_timeout(_call, 120s) ← OLLAMA CALL     │
│    → Exponential backoff: 1 retry (1s → 2s)          │
│    _log_timing("coder", CODER_MODEL_NAME)                │
│    → History logged                                         │
│    Release locks/semaphore                                 │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ 4. REVIEWER STAGE (pipeline.py:389-236)               │
│    _HEAVY_SEMAPHORE.acquire() ← VRAM GUARD          │
│    _get_profile_lock(profile_id).acquire() ← SERIALISM   │
│    _run_with_timeout(_call, 120s) ← OLLAMA CALL     │
│    → On error: Fallback to coder output                  │
│    _log_timing("reviewer", REVIEWER_MODEL_NAME)        │
│    → History logged                                         │
│    Release locks/semaphore                                 │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ 5. JUDGE STAGE (pipeline.py:392-275)                  │
│    IF JUDGE_ENABLED=False: Return dummy judge              │
│    _HEAVY_SEMAPHORE.acquire() ← VRAM GUARD          │
│    _get_profile_lock(profile_id).acquire() ← SERIALISM   │
│    _run_with_timeout(_call, 120s) ← OLLAMA CALL     │
│    _parse_judge_response(): Valid JSON, scores 1-10     │
│    → On error: Return scores = 0.0                      │
│    _log_timing("judge", JUDGE_MODEL_NAME)                │
│    → History logged                                         │
│    Release locks/semaphore                                 │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ 6. RISK ASSESSMENT (pipeline.py:395)                 │
│    assess_risk("code_generation", {code: reviewer_out}) │
│    → Returns risk_level (1.0-10.0) + tags           │
│    → Best-effort (failure → risk_level=1.0)           │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ 7. FULL TRACE LOGGING (pipeline.py:398-420)           │
│    history_logger.log({                                     │
│      mode: "code_smart",                                   │
│      original_prompt: user_prompt,                            │
│      final_output: reviewer_out,                              │
│      trace: {                                                 │
│        planner: {...},                                          │
│        worker: [{step: "coder_draft", ...},                    │
│                 {step: "reviewer_polish", ...}],               │
│        risk_assessment: {...},                                  │
│        judge: {...}                                            │
│      },                                                         │
│      risk: risk_out,                                         │
│      profile_id: profile_id                                    │
│    })                                                           │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ 8. JOB COMPLETION (pipeline.py:422)                     │
│    mark_job_done(job.id)                              │
│    → Release profile slot (_ACTIVE_BY_PROFILE)            │
│    → Decrement VRAM count (_ACTIVE_HEAVY_COUNT)       │
│    → Next job in queue can proceed                        │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
                    RETURN {
                      final_code: reviewer_out,
                      judge: {confidence, conflict, summary},
                      risk: {level, tags, reasons}
                    }
Where Queues Apply:
•	Step 1: Job enqueued → waits in per-profile queue
•	Step 2: try_acquire_next_job → blocks until VRAM slot available
Where Semaphores Apply:
•	Steps 3, 4, 5: _HEAVY_SEMAPHORE.acquire() before each model call
Timeout Values:
•	Queue wait: NO TIMEOUT (infinite polling)
•	Ollama calls: 120s per stage, 1 retry (exponential: 1s → 2s)
3.2 Chat Pipeline End-to-End
Entry: /api/chat → run_chat_smart()
┌─────────────────────────────────────────────────────────────┐
│ 1. JOB ENQUEUE (chat_pipeline.py:263-270)              │
│    enqueue_job(profile_id, kind="chat_smart", is_heavy=True)│
│    → Job created, added to per-profile FIFO queue          │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ 2. VRAM SLOT ACQUISITION (chat_pipeline.py:272-293)   │
│    try_acquire_next_job(profile_id)                     │
│    → Polling loop: max_wait=300s (5 MINUTES)          │
│    → If job lost/failed/timeout: Return error            │
│    → If state="running": Break and proceed                │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ 3. PLANNER STAGE (chat_pipeline.py:320-394)             │
│    _SMART_CHAT_SEMAPHORE.acquire() ← VRAM GUARD         │
│    Build prompt: CHAT_SYSTEM_PROMPT + plan() + context │
│    _run_with_timeout(_call_planner, 120s)             │
│    → Exponential backoff: 1 retry                          │
│    JSON extraction: {title, plan, notes}                   │
│    → On error: plan_text = planner_raw or ""              │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ 4. ANSWER STAGE (chat_pipeline.py:396-452)              │
│    Build prompt: plan + conversation + context            │
│    _run_with_timeout(_call_answer, 120s)               │
│    → Exponential backoff: 1 retry                          │
│    → On error: Try fallback_prompt (system prompt only)     │
│    → If still fails: Return error string                     │
│    _SMART_CHAT_SEMAPHORE.release() ← VRAM RELEASE       │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ 5. JUDGE STAGE (chat_pipeline.py:458)                  │
│    _run_chat_judge(): No semaphore (lightweight)         │
│    _run_with_timeout(_call_judge, 120s)              │
│    → JSON extraction: confidence, conflict, summary         │
│    → On error: Return with parse_error field              │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ 6. JOB COMPLETION (chat_pipeline.py:461-467)            │
│    IF answer_error: mark_job_failed(job.id)             │
│    ELSE: mark_job_done(job.id)                         │
│    → Release VRAM slot                                   │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ 7. FULL TRACE LOGGING (chat_pipeline.py:472-503)        │
│    history_logger.log({                                     │
│      mode: "chat_smart",                                   │
│      original_prompt: user_prompt,                            │
│      final_output: answer_text,                              │
│      trace: {                                                 │
│        planner: {title, content, confidence, ...},           │
│        worker: [{step: "response_generation", ...}],         │
│        risk_assessment: {level: 1.0, ...},                │
│        judge: {confidence, conflict, ...}                    │
│      },                                                         │
│      risk: {level: 1.0},                                   │
│      chat_id: chat_id,                                      │
│      profile_id: profile_id                                    │
│    })                                                           │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
                    RETURN {
                      answer: answer_text,
                      plan: plan_text,
                      judge: {...},
                      model_used: model_name,
                      planner_error: str|None,
                      answer_error: str|None
                    }
Where Queues Apply:
•	Step 1: Job enqueued → waits in per-profile queue
•	Step 2: try_acquire_next_job → blocks until VRAM slot available (max 300s)
Where Semaphores Apply:
•	Steps 3-4: _SMART_CHAT_SEMAPHORE.acquire() wraps planner + answer
•	Step 5: No semaphore (judge is lightweight JSON parsing)
Timeout Values:
•	Queue wait: 300s (5 minutes)
•	Ollama calls: 120s per stage, 1 retry (exponential: 1s → 2s)
3.3 Vision Pipeline
Entry: /api/vision → run_vision()
┌─────────────────────────────────────────────────────────────┐
│ 1. MODE SELECTION (vision_pipeline.py:130)               │
│    effective_model = VISION_MODEL_NAME (llava-phi3)    │
│    safe_prompt = _clamp_prompt(user_prompt)               │
│    → Max 4000 characters                                   │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ 2. JOB ENQUEUE (vision_pipeline.py:135-136)              │
│    enqueue_job(profile_id, kind="vision", is_heavy=True)  │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ 3. VRAM SLOT ACQUISITION (vision_pipeline.py:139-152)   │
│    try_acquire_next_job(profile_id)                     │
│    → Polling loop: max_wait=300s (5 MINUTES)          │
│    → If timeout: Return error message                     │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ 4. EXECUTION (vision_pipeline.py:154-184)                │
│    _VISION_SEMAPHORE.acquire() ← VRAM GUARD           │
│    _run_with_timeout(_call, 120s) ← OLLAMA CALL     │
│    → POST to {OLLAMA_URL}/api/generate with images=[b64]│
│    → Mode-based prompt prefixes:                             │
│      - describe: "Briefly describe this image..."            │
│      - ocr: "Extract all readable text..."                 │
│      - code: "Explain only relevant technical details"      │
│      - debug: "Carefully describe visible errors..."         │
│    → On error: Return "Vision stage failed: {error}"     │
│    _VISION_SEMAPHORE.release() ← VRAM RELEASE           │
│    mark_job_done(job.id)                                  │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
                        RETURN final_text
Where Queues Apply:
•	Step 2: Job enqueued → waits in per-profile queue
•	Step 3: try_acquire_next_job → blocks until VRAM slot available (max 300s)
Where Semaphores Apply:
•	Step 4: _VISION_SEMAPHORE.acquire() wraps vision model call
Timeout Values:
•	Queue wait: 300s (5 minutes)
•	Ollama call: 120s, 1 retry (exponential: 1s → 2s)
3.4 Tools Runtime Execution Path
Entry: Tool invocations → execute_tool()
┌─────────────────────────────────────────────────────────────┐
│ 1. TOOL LOOKUP (tools_runtime.py:310-325)                 │
│    tool = TOOLS_REGISTRY.get(name)                       │
│    → If None: Return error "Tool not registered"         │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ 2. RISK ASSESSMENT (tools_runtime.py:269-285)            │
│    assess_risk("tool", {tool, args, context})          │
│    → Returns risk_level (1.0-10.0) + tags           │
│    → Best-effort (failure → risk_level=1.0)           │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ 3. SECURITY EVALUATION (tools_runtime.py:288-202)            │
│    _compute_security_decision()                             │
│    → SecurityEngine.shared().evaluate(...)                  │
│    → Risk → auth_level mapping (see Section 5)               │
│    → Tool/operation overrides apply                         │
│    → On exception: Return BLOCK (fail-closed)              │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ 4. ENFORCEMENT CHECK (tools_runtime.py:333-391)             │
│    IF SECURITY_ENFORCEMENT_MODE = "off":                   │
│      → SKIP enforcement, proceed to execution               │
│    IF SECURITY_ENFORCEMENT_MODE = "soft":                  │
│      → IF auth_level >= 4:                               │
│        session_ok = consume_security_session_if_allowed(...)   │
│        → IF no session:                                   │
│          Return error: "security_approval_required"            │
│    IF SECURITY_ENFORCEMENT_MODE = "strict":                │
│      → IF auth_level >= 4:                                   │
│        Return error: "security_blocked"                         │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ 5. TOOL EXECUTION (tools_runtime.py:394-412)               │
│    NO QUEUE (bypasses queue_manager)                   │
│    future = _TOOL_EXECUTOR.submit(tool.func, args, context)│
│    result = future.result(timeout=60s) ← TIMEOUT            │
│    → On FuturesTimeoutError: error="tool_timeout"         │
│    → On exception: error="{type}: {message}"              │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ 6. HISTORY LOGGING (tools_runtime.py:430-458)              │
│    IF TOOLS_RUNTIME_LOGGING=True:                        │
│      logged_result = clamp_tool_output(result)              │
│      history_logger.log({                                 │
│        mode: "tool_execution",                             │
│        tool_record: {name, args, result, error, ...},    │
│        risk: risk_info,                                    │
│        security: security_info                               │
│      })                                                       │
└─────────────────────────┬───────────────────────────────────┘
                          │
                          ▼
                    RETURN {
                      ok: bool,
                      result: Any|None,
                      error: str|None,
                      risk: {...},
                      security: {...},
                      requires_approval: bool
                    }
Where Queues Apply:
•	NONE - Tools bypass queue_manager (lightweight operations)
Where Semaphores Apply:
•	NONE - Uses ThreadPoolExecutor (max 4 workers)
Timeout Values:
•	Tool execution: 60s, NO retries
________________________________________
4. Concurrency & Backpressure Model
4.1 Job Queue System (Primary Backpressure)
State Tracking:
_PROFILE_QUEUES: Dict[str, Deque[int]]    # Per-profile FIFO queues
_JOBS: Dict[int, Job]                    # Global job registry
_ACTIVE_BY_PROFILE: Dict[str, int]         # Which job_id is running per profile
_ACTIVE_HEAVY_COUNT: int                 # Global heavy job counter
Two-Level Backpressure:
Level 1: Per-Profile Serialism (FIFO)
# In try_acquire_next_job():
active_id = _ACTIVE_BY_PROFILE.get(profile_id)
if active_id is not None and active_id in _JOBS:
    return None  # This profile is busy, wait in queue
Why: Prevents concurrent operations from corrupting profile state (e.g., chat messages interleaving)
Level 2: Global VRAM Limit
if job.is_heavy:
    if _ACTIVE_HEAVY_COUNT >= MAX_CONCURRENT_HEAVY_REQUESTS:  # = 2
        return None  # VRAM full, all profiles wait
Why: Single 8GB GPU can't load >2 heavy models (7B DeepSeek/7B LLaVA)
Resource Release:
# In mark_job_done() / mark_job_failed():
_ACTIVE_BY_PROFILE.pop(profile_id, None)   # Release profile slot
_ACTIVE_HEAVY_COUNT = max(0, _ACTIVE_HEAVY_COUNT - 1)  # Release VRAM slot
# Next job in queue can now proceed
4.2 Semaphores (Secondary VRAM Protection)
Redundant Pattern (defense-in-depth or technical debt):
Semaphore	Location	Acquired Before	Released After
_HEAVY_SEMAPHORE	code/pipeline.py:203, 224, 245	Each model call	Lines 215, 236, 275
_SMART_CHAT_SEMAPHORE	chat/pipeline.py:320	Planner+Answer stages	Line 455
_VISION_SEMAPHORE	vision/pipeline.py:163	Vision model call	Line 181
All Use Same Limit: MAX_CONCURRENT_HEAVY_REQUESTS = 2
Deadlock Prevention (code_pipeline.py):
_HEAVY_SEMAPHORE.acquire()           # Acquire FIRST
profile_lock = _get_profile_lock(profile_id)
if profile_lock: profile_lock.acquire()  # Then acquire
# ... work ...
if profile_lock: profile_lock.release()  # Release profile lock FIRST
_HEAVY_SEMAPHORE.release()         # Then release semaphore
Why This Ordering: Prevents Thread A (holds semaphore, needs lock) vs Thread B (holds lock, needs semaphore) deadlock.
4.3 Thread Pools
Executor	Workers	Purpose	Timeout
_TOOL_EXECUTOR	4	Tool execution	60s per tool
_TIMEOUT_EXECUTOR	4	Ollama call timeouts	N/A (caller sets timeout)
VOICE_THREAD_POOL	3	Voice command processing	N/A
Executor Behavior:
•	Unbounded task queue (requests accumulate beyond worker count)
•	Workers process tasks sequentially
•	future.result(timeout=X) enforces per-task timeout
•	No automatic worker scaling (fixed pool size)
Why Fixed Worker Counts:
•	Predictable resource usage
•	Prevents thread explosion
•	Each worker has bounded memory overhead
4.4 Where Blocking is Intentional
Semaphore Blocks:
•	_HEAVY_SEMAPHORE.acquire(): Intentionally blocks thread until VRAM available
•	Prevents GPU memory overflow (hardware OOM is worse than waiting)
Profile Lock Blocks:
•	_get_profile_lock(profile_id).acquire(): Intentionally blocks other operations on same profile
•	Prevents state corruption (e.g., chat messages written in wrong order)
Queue Waiting:
•	Polling in try_acquire_next_job(): Intentionally waits until slot available
•	Ensures global VRAM limit is never exceeded
4.5 Where Blocking Could Be Avoidable
Polling-Based Job Waiting (code_pipeline.py:374-380):
while True:
    snapshot = get_job(job.id)
    if snapshot.state == "running":
        break
    time.sleep(0.1)  # Polling every 100ms
Why This Exists: Simple implementation, no event system
Better Approach: Condition variables or asyncio event notification (future work)
Impact: Wastes CPU cycles during queue waits (minor on low load, noticeable on high load)
4.6 Starvation Prevention
FIFO Guarantees:
•	collections.deque ensures strict ordering per profile
•	No priority mechanism that could cause starvation
•	All jobs are treated equally
Profile Fairness:
•	Each profile has its own queue
•	Global VRAM limit applies equally to all profiles
•	No profile can monopolize VRAM
Light Job Bypass (Unused):
•	If is_heavy=False, job bypasses global VRAM limit
•	Currently unused (all code/vision/chat default to is_heavy=True)
Future Enhancement: Dynamic VRAM limit based on actual GPU memory would allow >2 concurrent jobs for smaller models.
4.7 Resource Cleanup Guarantees
Queue Manager Shutdown (shutdown_queue_manager()):
1. Set _SHUTDOWN = True (stops queue event logging)
2. For all running jobs:
   - Set state = "failed"
   - Set error = "Shutdown: process terminated"
   - Set finished_ts
   - Release profile slot (_ACTIVE_BY_PROFILE)
   - Release VRAM slot (_ACTIVE_HEAVY_COUNT)
3. No new jobs can be accepted after shutdown
Thread Pool Shutdown:
# timeout_policy.py
atexit.register(_shutdown_timeout_executor)
def _shutdown_timeout_executor():
    _TIMEOUT_EXECUTOR.shutdown(wait=True)  # Blocks until all futures complete

# ghost_agent.py
global_cleanup():
    VOICE_THREAD_POOL.shutdown(wait=True)
Database Cleanup:
# history.py
atexit.register(_close_all_connections)
def _close_all_connections():
    for conn in _connection_pool:
        conn.close()
________________________________________
5. Security Model (Actual, Not Idealized)
5.1 Risk → Auth Level Mapping
Location: security_engine.py:296-319 (_auth_level_from_risk())
Exact Thresholds:
Risk Score Range	Auth Level	Enum	Meaning
≤ 1.9	ALLOW (1)	Safe - silent allow	
1.91 to 3.9	LOG_ONLY (2)	Low risk - allow and log	
3.91 to 5.9	CONFIRM (3)	Medium - require YES/NO popup	
5.91 to 7.9	CONFIRM_SENSITIVE (4)	Sensitive medium/high - confirm + elevated logging	
7.91 to 8.9	STRONG_VERIFY (5)	High risk - require password/phrase	
≥ 8.91	BLOCK (6)	Critical - block or require admin override	
All thresholds inclusive on lower bound: Risk=3.9 → LOG_ONLY, Risk=3.91 → CONFIRM
5.2 Authorization Levels 1-6
Location: security_engine.py:53-69 (SecurityAuthLevel enum)
Level	Name	UX Flow	Example Operations
1	ALLOW	No interaction	Reading files, safe API calls, simple calculations
2	LOG_ONLY	No interaction, logged	Reading config, listing directories
3	CONFIRM	Simple YES/NO popup	Writing files, creating directories, deletions
4	CONFIRM_SENSITIVE	Confirm + elevated logging	System config changes, destructive file operations
5	STRONG_VERIFY	Password/phrase challenge	Shell execution, sudo commands, system-level changes
6	BLOCK	Hard block	Factory reset, disk formatting, kernel modification
5.3 Session Consumption (Atomic Update)
Location: security_sessions.py:288-355 (consume_security_session_if_allowed())
Atomicity Mechanism:
UPDATE security_sessions
SET used_count = used_count + 1
WHERE id = (
    SELECT id
    FROM security_sessions
    WHERE profile_id = ?
      AND scope = ?
      AND auth_level >= ?
      AND expires_at > ?
      AND used_count < max_uses  ← Critical: Enforces use limit
    ORDER BY expires_at ASC, id ASC
    LIMIT 1
)
RETURNING id, profile_id, scope, ...
Guarantees:
•	No race conditions: Two concurrent calls cannot consume same session (SQL atomic update)
•	No overuse: Once used_count == max_uses, WHERE clause fails, no further consumption
•	Strict expiration: expires_at > NOW comparison prevents stale session usage
•	Oldest-first: Ordering consumes expiring sessions first (prevents hoarding)
Wildcard Approvals (security_sessions.py:459-478):
def sec_has_tool_wildcard(*, profile_id, required_level):
    return bool(cursor.fetchone())  # WHERE scope = 'tool:*'
•	Scope "tool:*" acts as global approval for all tools
•	Still requires matching profile_id
•	Still checks auth_level >= required_level
5.4 Fail-Closed vs Best-Effort Decisions
Fail-Closed (Security Engine) (tools_runtime.py:191-202):
try:
    sec = SecurityEngine.shared().evaluate(...)
except Exception as e:
    logger.error(f"SecurityEngine evaluation failed: {e}")
    return {
        "auth_level": int(SecurityAuthLevel.BLOCK),  # ← Fail-closed
        "reason": f"SecurityEngine evaluation failed: {e}; defaulting to BLOCK.",
        ...
    }
Best-Effort (Risk Assessment) (risk.py:212-219):
try:
    # Compute risk...
except Exception:
    return {
        "risk_level": 1.0,  # ← Fail-safe (minor risk)
        "tags": [],
        "reasons": "Risk assessment failed; defaulting to MINOR risk.",
        ...
    }
Why This Difference:
•	Security evaluation is enforcement point → Fail-closed prevents bypass
•	Risk assessment is observability only → Best-effort doesn't block execution
5.5 Enforcement Modes
Location: tools_runtime.py:333-391 and config.py:95
Mode	Behavior	When Auth Level ≥ 4
"off" (default)	No enforcement	Tools execute regardless of auth level
"soft"	Check for prior approval	consume_security_session_if_allowed() → If no session: Return "security_approval_required"
"strict"	Hard block	Return "security_blocked" immediately (no session check)
SECURITY_MIN_ENFORCED_LEVEL = 4: Enforcement only applies to CONFIRM_SENSITIVE (4) and above.
5.6 Where Security is Enforced vs Logged
Always Enforced:
•	Session consumption atomicity (SQLite guarantees)
•	SecurityEngine failures → BLOCK decision
•	Profile isolation in SQLite databases
Logged Only (Current Default):
•	Risk assessment tags (risk.py)
•	SecurityEngine decisions (stored in record["security"])
•	Tool execution attempts (tools_runtime.py history logging)
Enforced When Configured:
•	Tool execution above auth_level 4 (if SECURITY_ENFORCEMENT_MODE = "soft" or "strict")
•	Session consumption (if session exists and sufficient)
V3.6 Non-Blocking Signals (security_engine.py:387-442):
def security_evaluate_operation(*, profile_id, scope, required_level):
    # Check wildcard
    wildcard = get_best_session_for_scope(..., "tool:*", ...)
    if wildcard:
        return {"approval_required": False, "mode": "wildcard"}
    
    # Check specific session
    sess = get_best_session_for_scope(..., scope, ...)
    if sess:
        return {"approval_required": False, "mode": "session", "session_id": ...}
    
    # No approval
    return {"approval_required": True, "scope": scope, "required_level": required_level}
•	Used by UI/dashboard to show approval prompts
•	Never consumes sessions (non-destructive query)
•	Returns structured info only
5.7 Attack Surfaces and Mitigations
Attack Surface	Threat	Mitigation
Risk assessment bypass	Malicious payload evades heuristics	Risk is advisory only, enforcement uses SecurityEngine + sessions
Session race conditions	Concurrent tool calls share session	Atomic UPDATE ... RETURNING ensures single-use
Session expiry overrun	Stale sessions used after TTL	expires_at > NOW checked in WHERE clause
Authorization escalation	Low-level session used for high-risk op	auth_level >= required_level enforced
Security engine failure	SecurityEngine crashes	Exception caught → BLOCK decision (fail-closed)
Profile spoofing	Attacker provides different profile_id	profile_id from context, not user-controlled
Database corruption	SQLite modified/corrupted	WAL journal mode, 30s timeout, all queries try/except
Timing attacks	Learning about sessions via response time	Minor concern for local system, no artificial delays
________________________________________
6. Lifecycle Management
6.1 Startup Order and Dependencies
┌─────────────────────────────────────────────────────────────┐
│ ghost_agent.py Boot Sequence                                   │
├─────────────────────────────────────────────────────────────┤
│ 1. GPU DLL PATH SETUP (Lines 17-33)                     │
│    - Adds NVIDIA CUDA libraries to PATH                       │
│    - Non-fatal: prints warning on failure                  │
├─────────────────────────────────────────────────────────────┤
│ 2. BACKEND MODULE IMPORTS (Lines 35-47)                 │
│    - STTService, Planner, executor, run_chat_smart         │
│    - CRITICAL: sys.exit(1) if import fails - hard stop     │
├─────────────────────────────────────────────────────────────┤
│ 3. PROCESS SPAWNING (Lines 249-281)                     │
│    - server_proc: backend/code_server.py (subprocess)         │
│    - dash_proc: dashboard_app.py (subprocess, 2s delay)   │
│    - Non-fatal: continues even if processes fail             │
├─────────────────────────────────────────────────────────────┤
│ 4. AUDIO STREAM INIT (Lines 330-336)                      │
│    - sounddevice.InputStream with callback                   │
│    - Non-fatal: stream = None on failure                    │
├─────────────────────────────────────────────────────────────┤
│ 5. KEYBOARD LISTENER (Lines 338-339)                     │
│    - pynput.Listener for Ctrl+Alt+Space hotkey           │
├─────────────────────────────────────────────────────────────┤
│ 6. UI OVERLAY START (Lines 344-345)                      │
│    - Blocking tkinter.mainloop()                                │
└─────────────────────────────────────────────────────────────┘
Startup Dependencies:
•	Ghost agent cannot start without backend modules (hard stop on import failure)
•	Code server requires FastAPI, uvicorn
•	Dashboard is optional (guarded import, doesn't crash if missing)
6.2 Partial Initialization Safety
Guarded Imports with Safe Fallbacks:
Module	Guarded Dependency	Fallback Behavior
stt_service.py	faster_whisper.WhisperModel	WhisperModel = None; raises RuntimeError in _load_model()
automation/executor.py	call_ollama, execute_tool, SecurityEngine, history_logger	Set to None/defaults; check before use
tools/tools_runtime.py	TOOL_REGISTRY_PC_CONTROL	Set to {} - missing tools silently unavailable
Feature Flags (config.py):
JUDGE_ENABLED = True           # Disables judge stage if False
VISION_ENABLED = True         # Vision endpoints return error if False
STT_ENABLED = True           # STT returns error if False
TOOLS_RUNTIME_ENABLED = True  # Tools return disabled error if False
Dashboard (code_server.py:38-44):
try:
    from backend.modules.telemetry.dashboard import render_dashboard
except (ImportError, SyntaxError):
    def render_dashboard(...):
        return "<h2>Dashboard failed to load...</h2>"
    # Server continues, doesn't crash
6.3 Shutdown Guarantees
Global Cleanup (ghost_agent.py:283-317):
def global_cleanup():
    # Process cleanup
    if server_proc:
        server_proc.terminate()
        try: server_proc.wait(timeout=10)
        except subprocess.TimeoutExpired:
            server_proc.kill()  # Force kill if timeout
            server_proc.wait()
    
    if dash_proc:
        dash_proc.terminate()
        try: dash_proc.wait(timeout=10)
        except subprocess.TimeoutExpired:
            dash_proc.kill()
            dash_proc.wait()
    
    # Thread pool cleanup
    VOICE_THREAD_POOL.shutdown(wait=True)  # Waits for voice threads
    
    # Audio stream cleanup
    if stream:
        stream.stop()  # Stop receiving audio
        stream.close()  # Release audio device
    
    # Keyboard listener cleanup
    if listener:
        listener.stop()  # Stop hotkey listener
atexit Handlers (3 registered):
# timeout_policy.py:50
atexit.register(_shutdown_timeout_executor)
    → _TIMEOUT_EXECUTOR.shutdown(wait=True)

# history.py:336
atexit.register(_close_all_connections)
    → Closes SQLite connection pool (max 5 connections)

# queue_manager.py:434
atexit.register(shutdown_queue_manager)
    → Marks all running jobs as failed
    → Releases profile slots
    → Releases VRAM slots
Cleanup Triggers:
•	User closes overlay window → overlay.on_close() → global_cleanup() → sys.exit(0)
•	Ctrl+C in terminal → KeyboardInterrupt handler → global_cleanup() → sys.exit(0)
•	Normal process exit → atexit handlers run automatically
6.4 What Happens on Crashes or Interrupts
KeyboardInterrupt (Ctrl+C):
•	Ghost agent catches → global_cleanup() → sys.exit(0)
•	All resources released cleanly
Unhandled Exception (in modules):
•	Absorbed by try/except, returns error dicts
•	History logging failures caught and printed to stderr
•	Does not crash system
Process Death (kill -9):
•	NO CLEANUP: atexit handlers do not run
•	Child processes (server, dashboard) orphaned
•	SQLite WAL files may need recovery
•	VRAM locks released only when Python process dies (OS cleanup)
Mitigations for Process Death:
•	SQLite WAL journal mode enables automatic recovery
•	Orphaned child processes terminated by OS when parent dies
•	No persistent state corruption (all state in SQLite)
Resource Exhaustion Scenarios:
Scenario	System Behavior	Mitigation
VRAM OOM	MAX_CONCURRENT_HEAVY_REQUESTS=2 prevents >2 heavy jobs	Jobs wait in queue, system busy message
Database lock contention	SQLite timeout=30s on connections	Failed writes logged, don't crash
Thread pool exhaustion	Tool executor max 4 workers, tasks queue	New requests wait, no new threads spawned
________________________________________
7. Error Handling Philosophy
7.1 Which Errors Propagate (Raise)
Import Failures (Critical Dependencies Only):
•	backend/modules imports in ghost_agent.py (Lines 35-47)
•	sys.exit(1) if STTService, Planner, executor, run_chat_smart, get_profile fail to import
•	Rationale: These are core system dependencies; partial boot is worse than crash
JSON Parse Errors (Judge Response):
•	code/pipeline.py:_parse_judge_response() (Lines 291-354)
•	Raises ValueError on invalid JSON structure
•	Rationale: Judge output must be structurally valid; invalid output is a hard error
Timeout Errors (After Retries):
•	common/timeout_policy.py - run_with_retries()
•	After all retries exhausted, raises original exception
•	Rationale: Caller should know operation definitively failed
7.2 Which Errors Are Logged and Absorbed
"Must Never Raise" Modules (Safe for Calling from Error Handlers):
Module	Public Functions	Error Handling Strategy
history_logger.log()	log()	Absorbs all exceptions, prints to stderr
execute_tool()	execute_tool()	Always returns {"ok": False, "error": "..."}
run_coder(), run_reviewer(), run_judge()	run_*()	Return error strings/dicts, never raise
run_chat_smart()	run_chat_smart()	Returns structured dict with planner_error, answer_error
plan_and_execute()	plan_and_execute()	Returns structured dict with ok: bool, error: str
_log_*_timing()	Timing helpers	Wrapped in try/except pass - telemetry never blocks
Why These Absorb Errors:
•	Calling from error handlers → must not break error handling
•	Logging/telemetry is best-effort observability
•	Tool execution is user-triggered → better to show error than crash
7.3 Why Some Modules "Never Raise"
history_logger:
•	Used by all error handlers to log failures
•	If history logging fails, error handler itself could crash
•	Solution: Try/except around all history calls, print to stderr on failure
execute_tool:
•	Called from many contexts (chat UI, automation, tools API)
•	Raising exceptions would require try/except at every call site
•	Solution: Always return structured error dict
Pipeline Functions (run_coder, run_reviewer, run_judge):
•	LLM calls can timeout or fail for many reasons (network, OOM, bad prompt)
•	Raising would cause entire pipeline to abort
•	Solution: Return error strings, let caller decide (retry/fallback/abort)
7.4 How Observability Compensates for Best-Effort Paths
Full Trace Logging:
# code_pipeline.py:398-420
history_logger.log({
    mode: "code_smart",
    trace: {
        planner: {...},           # Planning attempt
        worker: [                # Each stage attempt
            {step: "coder_draft", result: "...", status: "ok"},
            {step: "reviewer_polish", result: "...", status: "error"}
        ],
        risk_assessment: {...},  # Risk calculation
        judge: {...}             # Final validation
    },
    risk: {...}                 # Final risk assessment
})
Why This Helps:
•	Even if stages fail, we log what happened
•	Dashboard can show chain of thought
•	Post-mortem debugging possible
•	User can see where failure occurred
Separate Timing Logs:
# Each stage logs timing independently
_log_timing("coder", model_name, duration, "ok", None)
_log_timing("reviewer", model_name, duration, "error", "network timeout")
_log_timing("judge", model_name, duration, "ok", None)
Why This Helps:
•	Performance monitoring across stages
•	Identify bottlenecks (LLM slow vs. parsing slow vs. risk slow)
•	Timeout tuning
________________________________________
8. Data & State
8.1 SQLite Usage Patterns
Common Pragmas Across All Databases:
PRAGMA journal_mode = WAL;        -- Write-Ahead Logging for crash safety
PRAGMA foreign_keys = ON;       -- Enable referential integrity
PRAGMA synchronous = NORMAL;      -- Balance safety and performance
PRAGMA timeout = 30;           -- Connection timeout
Connection Patterns:
Database	Connection Management	Pooling
chat_storage.py	Fresh connection per operation (with _get_conn() as conn:)	No
profile_kb.py	Fresh connection per operation	No
security_sessions.py	Fresh connection per operation	No
history.py	Reuse from pool	Yes (max 5 connections)
History Pool (history.py:48-114):
def _get_conn():
    # Try to reuse from pool
    for conn in _connection_pool:
        try: conn.execute("SELECT 1").fetchone()  # Health check
        return conn
    # Create new if needed
    conn = sqlite3.connect(DB_PATH, timeout=30.0)
    conn.row_factory = sqlite3.Row
    _connection_pool.append(conn)
    return conn

def _return_conn(conn):
    # Return to pool if not full
    if len(_connection_pool) < 5:
        # Return to pool
    else:
        conn.close()  # Close if pool full
8.2 What is Persistent vs Ephemeral
Persistent (Survives Crashes & Restarts):
Location	Type	Data	Backup/Recovery
data/chat/chat.db	SQLite	Profiles, chats, messages	WAL journal (automatic recovery)
data/profile_kb.sqlite3	SQLite	Snippets, embeddings	WAL journal
data/security_sessions.sqlite	SQLite	Active sessions	Designed to expire (no backup)
data/history/history.sqlite3	SQLite	All telemetry/history	WAL journal
data/history/history_*.jsonl	JSONL files	Rotated history	Dual-write (SQLite + JSONL) for redundancy
Ephemeral (Lost on Crash/Restart):
Module	State	Lifetime
queue_manager.py	_PROFILE_QUEUES, _JOBS, _ACTIVE_BY_PROFILE, _ACTIVE_HEAVY_COUNT	Process lifetime
planner.py	Singleton instance (model name)	Process lifetime
security_engine.py	Singleton instance (config)	Process lifetime
stt_service.py	Singleton + loaded WhisperModel	Process lifetime
tools_runtime.py	TOOLS_REGISTRY (global dict)	Process lifetime
history.py	_connection_pool	Process lifetime (atexit cleanup)
Intentionally Non-Persisted:
State	Module	Reason
Active jobs (queued/running)	Queue Manager	Jobs should be re-queued on restart; checkpointing adds complexity
Profile locks	Code Pipeline	Locks don't survive process death (recreates on demand)
STT model	STT Service	Reloading model on restart safer than corrupting state
8.3 Profile Isolation Model
Profile-Scoped Operations (Strong Isolation):
Module	Operations	Isolation Mechanism
chat_storage.py	profiles, chats, messages	WHERE id = ?, WHERE profile_id = ?, FK + CASCADE
profile_kb.py	snippets, search	WHERE profile_id = ? filter
security_sessions.py	create, consume, list	WHERE profile_id = ? AND scope = ?
queue_manager.py	enqueue, acquire	Per-profile FIFO queues, _ACTIVE_BY_PROFILE dict
Global Operations (Not Profile-Scoped):
Module	Operations	Scope
history.py	All log records	Global (no profile_id filter)
security_engine.py	evaluate, shared()	Stateless policy engine
feature_registry.py	All features	Global system capabilities
planner.py	plan_request	Uses profile_id only for logging/context
code_pipeline.py	run_coder, run_reviewer, run_judge	Optional profile_id, but operation is global
Profile ID Validation (Currently Weak):
profile_id = (profile_id or "").strip()
if not profile_id:
    raise ValueError("profile_id must be non-empty")
•	No database constraints linking across databases
•	No referential integrity checks in queue manager
•	security_sessions has no FK to chat_storage.profiles
Profile Isolation Guarantees:
✅ Strong Isolation:
•	Chat data: Complete isolation via foreign keys + CASCADE delete
•	Knowledge base: Profile-filtered queries
•	Security sessions: Profile-scoped lookups with wildcard support
⚠️ Moderate Isolation:
•	Queue manager: Per-profile FIFO but no DB validation
•	No cross-database FK constraints
❌ Missing Isolation:
•	No profile-based authentication at system level
•	No profile ownership validation for tool operations
•	Shared singleton resources (Planner, SecurityEngine) are global
________________________________________
9. Configuration & Defaults
9.1 Critical Configuration (System Cannot Work Without)
Variable	Type	Default	Why Critical
OLLAMA_URL	str	"http://127.0.0.1:11434"	All LLM calls fail without this
AVAILABLE_MODELS	dict	Varies	Model registry for lookups; empty registry = no models
CODER_MODEL_NAME, CHAT_MODEL_NAME, etc.	str	Derived	Used by pipelines; missing model = broken pipeline
Validation: None on startup - system fails at runtime if misconfigured.
9.2 Tunable Configuration (Safe to Adjust)
Variable	Type	Default	Impact	Safe Range
MAX_CONCURRENT_HEAVY_REQUESTS	int	2	VRAM concurrency	1-3 (depending on GPU VRAM)
OLLAMA_REQUEST_TIMEOUT_SECONDS	int	120	LLM call timeout	60-300s (depending on model speed)
TOOLS_MAX_RUNTIME_SECONDS	int	60	Tool execution timeout	30-120s (depending on typical tool runtime)
HISTORY_MAX_ENTRIES	int	1000	JSONL rotation	100-10000
9.3 Feature Flags
Variable	Default	Effect of Disabling
JUDGE_ENABLED	True	Judge stage skipped, returns dummy judge
ESCALATION_ENABLED	True	Auto-escalation on conflicts disabled
VISION_ENABLED	True	Vision endpoints return "Vision is disabled in config."
STT_ENABLED	True	STT returns error messages
TOOLS_RUNTIME_ENABLED	True	Tools return "Tools runtime disabled"
SECURITY_ENFORCEMENT_MODE	"off"	Security decisions logged only, not enforced
9.4 Model Configuration
Role	Config Key	Active Key	Actual Model
Coder	AVAILABLE_MODELS	ACTIVE_CODER_MODEL_KEY	"qwen2.5-coder:7b"
Reviewer	AVAILABLE_MODELS	ACTIVE_REVIEWER_MODEL_KEY	"deepseek-coder:6.7b"
Judge	AVAILABLE_MODELS	ACTIVE_JUDGE_MODEL_KEY	"deepseek-r1:7b"
Chat	N/A	N/A	"llama3.1:8b"
Smart Chat	N/A	N/A	"deepseek-r1:7b"
Vision	N/A	N/A	"llava-phi3:latest"
Embedding	N/A	N/A	"nomic-embed-text:latest"
9.5 Which Config Should Not Be Touched Casually
SECURITY_ENFORCEMENT_MODE:
•	Default "off" (security decisions logged only, no blocking)
•	Should default to "soft" for production use (enforce above auth_level 4)
•	Changing to "strict" blocks all high-risk operations permanently
MODEL_NAMES:
•	Changing CODER_MODEL_NAME requires ensuring model is in AVAILABLE_MODELS
•	Changing VISION_MODEL_NAME requires model with vision capability
•	Swapping to different model families may break assumptions (e.g., output format)
MAX_CONCURRENT_HEAVY_REQUESTS:
•	Increasing >2 on 8GB VRAM will cause GPU OOM
•	Should be based on actual GPU VRAM (e.g., 3-4 for 16GB GPU)
________________________________________
10. Known Tradeoffs & Deferred Work
10.1 Deferred Medium Issues
M1: Mixed Error Handling Within Same Module
•	File: backend/modules/automation/executor.py
•	Issue: plan() uses logger.exception(), but _execute_steps() and plan_and_execute() use silent string capture
•	Impact: Inconsistent observability
•	Deferred: Standardize on logger.exception() pattern
M2: Timing Log Key Name Inconsistency
•	File: backend/modules/tools/tools_runtime.py:101
•	Issue: Uses "tool": name instead of "stage": stage like other pipelines
•	Impact: Dashboard parsing must handle both keys
•	Deferred: Standardize on "stage" key
M3: History Logger Call Format Inconsistency
•	File: backend/modules/stt/stt_service.py:282-289
•	Issue: Uses history_logger.log(mode="stt", payload={...}) with kwargs
•	Issue: Other modules use history_logger.log({...}) with dict
•	Impact: Mixed calling conventions
•	Deferred: Standardize on dict format
10.2 Code Duplication (Low-Medium Priority)
D1: JSON Extraction Logic
•	Files: screen_locator.py:98-136, automation/executor.py:79, chat/pipeline.py:209, planner.py:189
•	Issue: 3-4 similar implementations of JSON extraction with regex
•	Deferred: Create backend/modules/common/json_utils.py with _extract_json(), _parse_json_response()
D2: Queue Management Pattern
•	Files: stt_service.py:250-264, code/pipeline.py:371-380, chat/pipeline.py:272-293, vision/pipeline.py:136-154
•	Issue: All 4 modules repeat identical enqueue → wait/poll → mark_done pattern
•	Deferred: Create queue wrapper helper for enqueue/acquire/wait/mark_done
D3: Text Clamping Implementations
•	Files: code/pipeline.py:52-62, common/io_guards.py:34-47
•	Issue: Two similar implementations with different truncation messages
•	Deferred: Consolidate into single implementation in io_guards.py
10.3 Inconsistent Queue Timeout Handling
Issue: code/pipeline.py:374-380 has no timeout (infinite polling)
•	Files: chat_pipeline.py, vision_pipeline.py have 300s timeout
•	Impact: Code pipeline waits indefinitely if job never starts
•	Deferred: Add timeout to code pipeline polling loop
10.4 Potential Improvements (Low Priority)
I1: Replace Polling with Condition Variables
•	Files: All pipeline waiting loops
•	Current: while True: snapshot = get_job(job.id); if running: break; time.sleep(0.1)
•	Better: Event-based notification when job state changes
•	Impact: Reduce CPU usage during queue waits
•	Deferred: V5.x or V6
I2: Add Metrics for Queue Depth and Contention
•	Currently: No visibility into queue lengths, semaphore wait times, thread pool utilization
•	Impact: Can't diagnose backpressure issues proactively
•	Deferred: V5.x or V6
I3: Standardize on logging.getLogger(__name__)
•	Files: stt_service.py, tts_service.py, screen_locator.py
•	Issue: Use string-based logger names instead of module name
•	Impact: Minor inconsistency when copy-pasting code
•	Deferred: Style guide update
I4: Replace datetime.utcnow()
•	Files: security_sessions.py:143, 396
•	Issue: datetime.utcnow() deprecated in Python 3.12+
•	Better: datetime.now(timezone.utc)
•	Impact: Future Python versions may deprecate
•	Deferred: V5.x maintenance
10.5 Test Coverage Gaps (Non-Blocking)
Untested Modules:
•	Chat pipeline (1,137 lines)
•	Security engine (442 lines)
•	Jobs queue manager (433 lines)
•	Tools runtime (592 lines)
•	Automation executor (235 lines)
Coverage: ~20-25% of backend modules have tests Deferred: Post-V5 stabilization, V6
________________________________________
11. Mental Model for Future Work
11.1 How to Safely Extend the System
Adding a New Tool:
1.	Define function with signature: func(args: Dict, context: Dict) -> Dict
2.	Decorate with @register_tool(name, description, risk_level)
3.	Implement {"ok": True/False, "result": ..., "error": ...} return format
4.	Add to tool registry via auto-registration or manual import
5.	Tool automatically inherits: timeout (60s), threading (executor), security (session check), history logging
Adding a New Pipeline Stage (e.g., for code pipeline):
1.	Define run_<stage>(user_prompt: str, profile_id: str) -> str|dict
2.	Wrap in _HEAVY_SEMAPHORE.acquire() / release() for VRAM protection
3.	Wrap in _get_profile_lock(profile_id).acquire() / release() for profile serialism
4.	Call via _run_with_timeout() for Ollama timeout + retries
5.	Log timing via _log_timing()
6.	Integrate into pipeline sequence with error handling
Adding a New Module:
1.	Create directory backend/modules/<name>/
2.	Add __init__.py with module-level docstring
3.	Import optional dependencies with try/except, set fallbacks to None
4.	Use logging.getLogger(__name__) for consistent logging
5.	Use history_logger.log() for telemetry (if relevant)
6.	Register with feature registry for capabilities tracking
11.2 How to Reason About Changes
VRAM Impact:
•	Question: Does this change increase concurrent heavy model loads?
•	If yes: Must respect MAX_CONCURRENT_HEAVY_REQUESTS
•	Check: Does it bypass queue manager? (tools_runtime does, intentionally)
•	Check: Does it use _HEAVY_SEMAPHORE? (all heavy operations should)
Profile Isolation:
•	Question: Does this change cross profile boundaries?
•	If yes: Document why, ensure intentional
•	Check: Does it respect per-profile FIFO in queue?
•	Check: Does it use _get_profile_lock()?
Error Propagation:
•	Question: Can this new code raise an exception that breaks the system?
•	If yes: Consider absorbing and returning error dict instead
•	Check: Should history logging be wrapped in try/except?
•	Check: Is this called from error handlers? If yes, MUST NOT raise
Security Impact:
•	Question: Does this change bypass security evaluation?
•	If yes: Document why (e.g., best-effort vs enforcement)
•	Check: Does risk assessment need to be called?
•	Check: Does tool execution need to respect SECURITY_ENFORCEMENT_MODE?
Concurrency Impact:
•	Question: Does this introduce new shared state?
•	If yes: Add lock/semaphore
•	Check: Lock ordering to prevent deadlocks (acquire broad before narrow)
•	Check: Always release locks/semaphores in finally blocks
11.3 Where Bugs Are Most Likely to Appear
Deadlock Zones:
•	New code adding lock acquisition in wrong order
•	Semaphore + profile lock ordering (code_pipeline has correct pattern, copy it)
•	Lock held across blocking I/O (e.g., database writes, network calls)
Race Condition Zones:
•	New code reading/writing shared state without lock
•	Queue manager state (_ACTIVE_BY_PROFILE, _ACTIVE_HEAVY_COUNT)
•	Singleton initialization (use threading.Lock() like Planner, SecurityEngine)
Resource Leak Zones:
•	Locks/semaphores not released in finally
•	Database connections not closed
•	Subprocesses not terminated on shutdown
•	File handles not closed
Error Swallowing Zones:
•	Catch-all except Exception without logging
•	Returning error dicts without propagating context
•	History logging failures hidden (should print to stderr)
Configuration Assumption Zones:
•	Assuming feature flag is enabled without checking
•	Assuming model name exists without validation
•	Hardcoding paths or URLs instead of using config
11.4 How Not to Break Invariants
Don't Break Local-First:
•	Never add hardcoded cloud API as dependency
•	Never assume network availability
•	Make cloud features optional with graceful fallback
Don't Break Determinism:
•	Never add hidden heuristics without documentation
•	Never use learned models outside LLM
•	Keep all scoring logic readable and explicit
Don't Break Human-in-the-Loop:
•	Never execute tools/automation without execute=True explicitly passed
•	Never bypass security sessions in enforcement mode
•	Never make LLM calls that modify system without approval
Don't Break Replaceability:
•	Never couple module A directly to module B without abstraction
•	Use dependency injection or dynamic imports (like router does)
•	Keep layer boundaries clear (input → routing → planning → execution → memory)
Don't Break Observability:
•	Never add state changes without history logging
•	Never skip timing logs for new stages
•	Keep trace structure consistent (planner, worker, risk, judge)
________________________________________
This document reflects the final stabilized V5 codebase and serves as the authoritative internal reference for understanding and evolving the system.

